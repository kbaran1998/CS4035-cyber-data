{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imbalance task - Rank Swapping\n",
    "\n",
    "#### Author: Michał Okoń"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date # datetime works too\n",
    "from currency_converter import CurrencyConverter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count: 288\n"
     ]
    }
   ],
   "source": [
    "# If this cell does not work try running ``\n",
    "\n",
    "import io\n",
    "import nbformat\n",
    "\n",
    "nb = nbformat.read(\"Lab1_Michal_Okon.ipynb\", nbformat.NO_CONVERT)\n",
    "word_count = 0\n",
    "for cell in nb.cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "\n",
    "print(\"Word count:\", word_count)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rank-swapping algorithm implementation\n",
    "In the implementation of the rank-swapping algorithm, I will be swapping the values of the numerical and ordinal features in our code. Moreover, I have added several new aggregated features to better utilize the rank swap algorithm. That is, I will be swapping amount of euros transferred in the transaction and the variables that related to the number of times certain ip/cards/emails have been used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def rank_swap_custom(df, features_to_swap, swap_range=0.02):\n",
    "    \"\"\"\n",
    "    Rank-swaps the values of the features in the dataframe.\n",
    "    :param df: dataframe to be swapped\n",
    "    :param features_to_swap: features to be swapped\n",
    "    :param swap_range: swap range as a percentage of the dataframe length\n",
    "    :return: swapped dataframe\n",
    "    \"\"\"\n",
    "    # Select the swap range\n",
    "    swap_range = int(swap_range * len(df))\n",
    "\n",
    "    df_swapped = df.reset_index(drop=False)\n",
    "    for feature in features_to_swap:\n",
    "\n",
    "        df_swapped = df_swapped.sort_values(by=feature)\n",
    "        df_swapped = df_swapped.reset_index(drop=True)\n",
    "\n",
    "        # Swap the values of the feature\n",
    "        for i in range(len(df)):\n",
    "            swap_index = random.randint(max(0, i - swap_range), min(i + swap_range, len(df)-1))\n",
    "            df_swapped.at[i, feature], df_swapped.at[swap_index, feature] = df_swapped.at[swap_index, feature], df_swapped.at[i, feature]\n",
    "\n",
    "    df_swapped.set_index('Id', inplace=True, drop=True)\n",
    "    df_swapped = df_swapped.sort_index()\n",
    "    return df_swapped"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def advanced_feature_eng(df):\n",
    "    \"\"\"\n",
    "    Add new features to the dataframe related to the number of times certain ip/cards/emails have been used.\n",
    "    :param df: dataframe\n",
    "    :return: modified dataframe\n",
    "    \"\"\"\n",
    "    # Check if the shopper's country of origin is the same as the issuer country\n",
    "    df['countries_equal'] = (df['shoppercountrycode'] == df['issuercountrycode'])\n",
    "    df.loc[df['countries_equal'] == False, 'countries_equal'] = 0\n",
    "    df.loc[df['countries_equal'] == True, 'countries_equal'] = 1\n",
    "\n",
    "    # Check if the shopper's country of origin is the same as the currency\n",
    "    df['currency_equal'] = (df['shoppercountrycode'] == df['currencycode'])\n",
    "    df.loc[df['currency_equal'] == False, 'currency_equal'] = 0\n",
    "    df.loc[df['currency_equal'] == True, 'currency_equal'] = 1\n",
    "\n",
    "    # Check with how many cards an email has been used\n",
    "    df['card_email_count'] = df.groupby('mail_id')['card_id'].transform('count')\n",
    "\n",
    "    # Check with how many cards an ip has been used\n",
    "    df['card_ip_count'] = df.groupby('ip_id')['card_id'].transform('count')\n",
    "\n",
    "    # Check with how many emails a card has been used\n",
    "    df['email_card_count'] = df.groupby('card_id')['mail_id'].transform('count')\n",
    "\n",
    "    # Check with how many ips a card has been used\n",
    "    df['ip_card_count'] = df.groupby('card_id')['ip_id'].transform('count')\n",
    "\n",
    "    # Check with how many emails an ip has been used\n",
    "    df['email_ip_count'] = df.groupby('ip_id')['mail_id'].transform('count')\n",
    "\n",
    "    # Check with how many ips an email has been used\n",
    "    df['ip_email_count'] = df.groupby('mail_id')['ip_id'].transform('count')\n",
    "\n",
    "    # Check how many times a card has been used\n",
    "    df['card_count'] = df.groupby('card_id')['card_id'].transform('count')\n",
    "\n",
    "    # Check how many times an email has been used\n",
    "    df['email_count'] = df.groupby('mail_id')['mail_id'].transform('count')\n",
    "\n",
    "    # Check how many times an ip has been used\n",
    "    df['ip_count'] = df.groupby('ip_id')['ip_id'].transform('count')\n",
    "\n",
    "    # Check how many times a bin has been used\n",
    "    df['bin_count'] = df.groupby('bin')['bin'].transform('count')\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "def country_equal_feature_eng(df):\n",
    "    # Check if the shopper's country of origin is the same as the issuer country\n",
    "    df['countries_equal'] = (df['shoppercountrycode'] == df['issuercountrycode'])\n",
    "    df.loc[df['countries_equal'] == False, 'countries_equal'] = 0\n",
    "    df.loc[df['countries_equal'] == True, 'countries_equal'] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "class CustomDataTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self, currency_conv_func,\n",
    "        feature_engineering_func=country_equal_feature_eng,\n",
    "        card_enc=LabelEncoder(),\n",
    "        ip_enc=LabelEncoder(),\n",
    "        country_code_enc=LabelEncoder(),\n",
    "        tx_variant_code_enc=LabelEncoder(),\n",
    "        currency_code_enc=LabelEncoder(),\n",
    "        shopper_interaction_enc=LabelEncoder(),\n",
    "        account_code_enc=LabelEncoder(),\n",
    "        card_verif_code_supplied=LabelEncoder(),\n",
    "        mail_id_enc=LabelEncoder(),\n",
    "        bin_encoder=LabelEncoder(),\n",
    "        scaler=StandardScaler(),\n",
    "\n",
    "        drop_private_columns=False,\n",
    "        columns_to_scale=None,\n",
    "        one_hot_columns=None\n",
    "\n",
    "    ):\n",
    "        '''\n",
    "\n",
    "        :param currency_conv_func: currency conversion function\n",
    "        :param feature_engineering_func: feature engineering function\n",
    "        :param card_enc: card encoder\n",
    "        :param ip_enc: ip encoder\n",
    "        :param country_code_enc:  country code encoder\n",
    "        :param tx_variant_code_enc: tx variant code encoder\n",
    "        :param currency_code_enc:  currency code encoder\n",
    "        :param shopper_interaction_enc:  shopper interaction encoder\n",
    "        :param account_code_enc: account code encoder\n",
    "        :param card_verif_code_supplied: card verification code supplied encoder\n",
    "        :param mail_id_enc: mail id encoder\n",
    "        :param scaler: scaler used\n",
    "        :param drop_private_columns: drop columns that might be considered sensitive data (e.g. mail_id)\n",
    "        :param columns_to_scale: columns that will be scaled using the scaler\n",
    "        :param one_hot_columns: columns that will be one-hot encoded\n",
    "        '''\n",
    "        self.card_enc = card_enc\n",
    "        self.ip_enc = ip_enc\n",
    "        self.country_code_enc = country_code_enc\n",
    "        self.tx_variant_code_enc = tx_variant_code_enc\n",
    "        self.currency_code_enc = currency_code_enc\n",
    "        self.shopper_interaction_enc = shopper_interaction_enc\n",
    "        self.account_code_enc = account_code_enc\n",
    "        self.bin_enc = bin_encoder\n",
    "        self.card_verif_code_supplied = card_verif_code_supplied\n",
    "        self.mail_id_enc = mail_id_enc\n",
    "\n",
    "        self.currency_conv_func = currency_conv_func\n",
    "        self.feature_eng_func = feature_engineering_func\n",
    "\n",
    "        self.country_codes = None\n",
    "        self.cleaned_data = None\n",
    "\n",
    "        self.scaler = scaler\n",
    "        if columns_to_scale is None:\n",
    "            self.columns_to_scale = []\n",
    "        else:\n",
    "            self.columns_to_scale = columns_to_scale\n",
    "        if one_hot_columns is None:\n",
    "            self.one_hot_columns = []\n",
    "        else:\n",
    "            self.one_hot_columns = one_hot_columns\n",
    "\n",
    "        self.drop_private_columns = drop_private_columns\n",
    "\n",
    "    def _clean_data(self, df):\n",
    "        # Cleaning up data inconsisstencies\n",
    "        df.loc[df['cardverificationcodesupplied'].isna(), 'cardverificationcodesupplied'] = False\n",
    "        # df.loc[df['mail_id'].str.contains('na', case=False), 'mail_id'] = 'email99999'\n",
    "        df.loc[df['cvcresponsecode'] > 2, 'cvcresponsecode'] = 3\n",
    "\n",
    "        df.loc[df['issuercountrycode'].isna(), 'issuercountrycode'] = '--'\n",
    "        df.loc[df['shoppercountrycode'].isna(), 'shoppercountrycode'] = '--'\n",
    "        unique_issuer_cc = df['issuercountrycode'].unique()\n",
    "        unique_shopper_cc = df['shoppercountrycode'].unique()\n",
    "        both = np.append(unique_issuer_cc, unique_shopper_cc)\n",
    "        df_countrycodes = pd.DataFrame(both)\n",
    "        unique_country_codes = df_countrycodes[0].unique()\n",
    "        self.country_codes = unique_country_codes\n",
    "\n",
    "        df['amount_eur'] = df.apply(lambda x: self.currency_conv_func(x), axis=1)\n",
    "        df.drop(\"amount\", axis=1, inplace=True)\n",
    "        df['accountcode'] = df['accountcode'].apply(lambda x: re.sub('Account','',x))\n",
    "        df.loc[(df['accountcode'] == 'UK'),'accountcode'] = 'GB'\n",
    "        df.loc[(df['accountcode'] == 'Mexico'),'accountcode'] = 'MX'\n",
    "        df.loc[(df['accountcode'] == 'Sweden'),'accountcode'] = 'SE'\n",
    "        df.loc[(df['accountcode'] == 'APAC'),'accountcode'] = 'APAC'\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        df = X.copy(deep=True)\n",
    "        df = self._clean_data(df)\n",
    "\n",
    "        # 1.Card ID\n",
    "        self.card_enc.fit(df['card_id'])\n",
    "\n",
    "\n",
    "        # 2.IP ID\n",
    "        self.ip_enc.fit(df['ip_id'])\n",
    "\n",
    "        # 3. Country code\n",
    "        self.country_code_enc.fit(self.country_codes)\n",
    "\n",
    "        # 4. TX variant code\n",
    "        self.tx_variant_code_enc.fit(df['txvariantcode'])\n",
    "\n",
    "        # 5. Currency code\n",
    "        self.currency_code_enc.fit(df['currencycode'])\n",
    "\n",
    "        # 6. Shopper Interaction\n",
    "        self.shopper_interaction_enc.fit(df['shopperinteraction'])\n",
    "\n",
    "        # 7. Account code\n",
    "        self.account_code_enc.fit(df['accountcode'])\n",
    "        # df['accountcode'] = self.account_code_enc.transform(df.accountcode)\n",
    "\n",
    "        # 8. Card Verification Code Supplied\n",
    "        self.card_verif_code_supplied.fit(df['cardverificationcodesupplied'])\n",
    "        # df['cardverificationcodesupplied'] = self.card_verif_code_supplied.transform(\n",
    "        #     df.cardverificationcodesupplied\n",
    "        # )\n",
    "\n",
    "        # 9. Email ID\n",
    "        self.mail_id_enc.fit(df['mail_id'])\n",
    "\n",
    "        # 10. Bin\n",
    "        self.bin_enc.fit(df['bin'])\n",
    "        # df['mail_id'] = self.mail_id_enc.transform(df.mail_id)\n",
    "        # df.drop(\"mail_id\", axis=1, inplace=True)\n",
    "        #\n",
    "        # # Fit the scaler\n",
    "        # if self.scaler is not None:\n",
    "        #     self.scaler.fit(df)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy(deep=True)\n",
    "        df = self._clean_data(df)\n",
    "\n",
    "        # Feature Engineering\n",
    "        if self.feature_eng_func is not None:\n",
    "            df = self.feature_eng_func(df)\n",
    "\n",
    "        # 1.Card ID\n",
    "        if self.drop_private_columns:\n",
    "            df.drop(\"card_id\", axis=1, inplace=True)\n",
    "        else:\n",
    "            df['card_id'] = self.card_enc.transform(df.card_id)\n",
    "\n",
    "        # 2.IP ID\n",
    "        # df['ip_id'] = self.ip_enc.transform(df.ip_id)\n",
    "        if self.drop_private_columns:\n",
    "            df.drop(\"ip_id\", axis=1, inplace=True)\n",
    "        else:\n",
    "            df['ip_id'] = self.ip_enc.transform(df.ip_id)\n",
    "        # 3. Country code\n",
    "        if 'issuercountrycode' not in self.one_hot_columns:\n",
    "            df['issuercountrycode'] = self.country_code_enc.transform(df.issuercountrycode)\n",
    "        if 'shoppercountrycode' not in self.one_hot_columns:\n",
    "            df['shoppercountrycode'] = self.country_code_enc.transform(df.shoppercountrycode)\n",
    "\n",
    "        # 4. TX variant code\n",
    "        if 'txvariantcode' not in self.one_hot_columns:\n",
    "            df['txvariantcode'] = self.tx_variant_code_enc.transform(df.txvariantcode)\n",
    "\n",
    "        # 5. Currency code\n",
    "        if 'currencycode' not in self.one_hot_columns:\n",
    "            df['currencycode'] = self.currency_code_enc.transform(df.currencycode)\n",
    "\n",
    "        # 6. Shopper Interaction\n",
    "        if 'shopperinteraction' not in self.one_hot_columns:\n",
    "            df['shopperinteraction'] = self.shopper_interaction_enc.transform(\n",
    "                df.shopperinteraction\n",
    "            )\n",
    "\n",
    "        # 7. Account code\n",
    "        if 'accountcode' not in self.one_hot_columns:\n",
    "            df['accountcode'] = self.account_code_enc.transform(df.accountcode)\n",
    "\n",
    "        # 8. Card Verification Code Supplied\n",
    "        if 'cardverificationcodesupplied' not in self.one_hot_columns:\n",
    "            df['cardverificationcodesupplied'] = self.card_verif_code_supplied.transform(df['cardverificationcodesupplied'])\n",
    "\n",
    "        # 9. Email ID\n",
    "        if self.drop_private_columns:\n",
    "            df.drop(\"mail_id\", axis=1, inplace=True)\n",
    "        else:\n",
    "            df['mail_id'] = self.mail_id_enc.transform(df.mail_id)\n",
    "\n",
    "        # 10. Bin\n",
    "        if self.drop_private_columns:\n",
    "            df.drop(\"bin\", axis=1, inplace=True)\n",
    "        else:\n",
    "            df['bin'] = self.bin_enc.transform(df.bin)\n",
    "        # df.drop(\"bin\", axis=1, inplace=True)\n",
    "\n",
    "        # Scale the data\n",
    "        if self.scaler is not None and len(self.columns_to_scale) > 0:\n",
    "            # Columns to scale\n",
    "            scale_columns = np.array(df[self.columns_to_scale])\n",
    "            if len(scale_columns.shape) == 1:\n",
    "                scale_columns = scale_columns.reshape(-1, 1)\n",
    "                df[self.columns_to_scale] = self.scaler.fit_transform(scale_columns).reshape(-1)\n",
    "            df[self.columns_to_scale] = self.scaler.fit_transform(scale_columns)\n",
    "\n",
    "        if len(self.one_hot_columns) > 0:\n",
    "            df = self.one_hot_encode(df)\n",
    "        return df\n",
    "\n",
    "    def one_hot_encode(self, df):\n",
    "        # One hot encode the selected columns\n",
    "        cols_to_encode = df[self.one_hot_columns]\n",
    "        encoder = OneHotEncoder(sparse=False)\n",
    "        encoded_cols = encoder.fit_transform(cols_to_encode)\n",
    "        encoded_cols_df = pd.DataFrame(\n",
    "            encoded_cols, columns=encoder.get_feature_names(self.one_hot_columns), index=df.index\n",
    "        )\n",
    "        df.drop(self.one_hot_columns, axis=1, inplace=True)\n",
    "        df = pd.concat([df, encoded_cols_df], axis=1)\n",
    "\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "def train_classifier_and_plot(classifier, data_quad, cv_func=None):\n",
    "    X_train_data, X_test_data, y_train_data, y_test_data = data_quad\n",
    "    if cv_func is None:\n",
    "        classifier.fit(X_train_data, y_train_data)\n",
    "    else:\n",
    "        classifier = cv_func(X, y, classifier,\n",
    "                             scoring='accuracy',\n",
    "                             k=10\n",
    "        )\n",
    "\n",
    "    y_predictions = classifier.predict(X_test_data)\n",
    "    y_predictions_proba = classifier.predict_proba(X_test_data)[:, 1]\n",
    "\n",
    "    print(classification_report(y_test_data, y_predictions))\n",
    "    print(f\"Accuracy: {accuracy_score(y_test_data, y_predictions) * 100:.2f}\")\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test_data, y_predictions_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    ConfusionMatrixDisplay.from_predictions(y_test_data, y_predictions)\n",
    "    plt.show()\n",
    "    return fpr, tpr, roc_auc, classifier\n",
    "\n",
    "\n",
    "def plot_roc_curves(plot_title, *plot_tuples):\n",
    "    plt.clf()\n",
    "    for plot_tuple in plot_tuples:\n",
    "        plt.plot(plot_tuple[0], plot_tuple[1], label=plot_tuple[2])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\micha\\\\Documents\\\\Przegrane\\\\Delft\\\\masters\\\\CDA\\\\deliverable\\\\data\\\\train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m train_data_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mgetcwd(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m test_data_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(os\u001B[38;5;241m.\u001B[39mgetcwd(), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_data.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m train_data_df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m train_data_df\u001B[38;5;241m.\u001B[39mset_index(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m'\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      9\u001B[0m test_data_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(test_data_path)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    209\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    210\u001B[0m         kwargs[new_arg_name] \u001B[38;5;241m=\u001B[39m new_arg_value\n\u001B[1;32m--> 211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[0;32m    326\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    327\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39m_format_argument_list(allow_args)),\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[0;32m    329\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mfind_stack_level(),\n\u001B[0;32m    330\u001B[0m     )\n\u001B[1;32m--> 331\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    946\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[0;32m    947\u001B[0m )\n\u001B[0;32m    948\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 950\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    602\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    604\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 605\u001B[0m parser \u001B[38;5;241m=\u001B[39m TextFileReader(filepath_or_buffer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m    607\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    608\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1439\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1441\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1442\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1733\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1734\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1736\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1737\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1738\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1739\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1740\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1741\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1742\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1743\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\new_env\\lib\\site-packages\\pandas\\io\\common.py:856\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    851\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    852\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    855\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    863\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    864\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    865\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\micha\\\\Documents\\\\Przegrane\\\\Delft\\\\masters\\\\CDA\\\\deliverable\\\\data\\\\train_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data inside pandas Dataframe\n",
    "import os\n",
    "\n",
    "train_data_path = os.path.join(os.getcwd(), \"data\", \"train_data.csv\")\n",
    "test_data_path = os.path.join(os.getcwd(), \"data\", \"test_data.csv\")\n",
    "\n",
    "train_data_df = pd.read_csv(train_data_path)\n",
    "train_data_df.set_index('Id', inplace=True)\n",
    "test_data_df = pd.read_csv(test_data_path)\n",
    "test_data_df.set_index('Id', inplace=True)\n",
    "\n",
    "# Connect the train and test dataframes\n",
    "train_data_df['is_train'] = 1\n",
    "test_data_df['is_train'] = 0\n",
    "data_df = pd.concat([train_data_df, test_data_df], axis=0, sort=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The code below may take a while to. You can reduce the number of the features to swap to speed up the algorithm."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def conv(row, data_conv=date(2023, 1, 2)):\n",
    "    c = CurrencyConverter()\n",
    "    return c.convert(row['amount'], row['currencycode'], 'EUR', date=data_conv)\n",
    "\n",
    "# Regular pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('custom_transform', CustomDataTransformer(conv, feature_engineering_func=None,\n",
    "                                               columns_to_scale=['issuercountrycode', 'txvariantcode', 'bin', 'currencycode',\n",
    "       'shoppercountrycode', 'shopperinteraction',\n",
    "       'cardverificationcodesupplied', 'cvcresponsecode', 'accountcode',\n",
    "       'label', 'amount_eur', 'countries_equal', 'currency_equal',\n",
    "       'card_email_count', 'card_ip_count', 'email_card_count',\n",
    "       'ip_card_count', 'email_ip_count', 'ip_email_count', 'card_count',\n",
    "       'email_count', 'ip_count', 'bin_count'])),  # apply the custom function\n",
    "])\n",
    "pipeline = pipeline.fit(data_df)\n",
    "# Fit the pipeline\n",
    "transformed_data_df = pipeline.transform(data_df)\n",
    "transformed_train_data_df = transformed_data_df[transformed_data_df['is_train'] == 1].copy()\n",
    "transformed_train_data_df.drop(['is_train'], axis=1, inplace=True)\n",
    "transformed_test_data_df = transformed_data_df[transformed_data_df['is_train'] == 0].copy()\n",
    "transformed_test_data_df.drop(['is_train'], axis=1, inplace=True)\n",
    "transformed_test_data_df.drop(['label'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Perform swapping\n",
    "df_swapped = rank_swap_custom(transformed_train_data_df.copy(), features_to_swap=[\"amount_eur\", 'card_email_count', 'card_ip_count', 'email_card_count',\n",
    "       'ip_card_count', 'email_ip_count', 'ip_email_count', 'card_count',\n",
    "       'email_count', 'ip_count'], swap_range=0.05)\n",
    "transformed_train_data_df.head()\n",
    "X = transformed_train_data_df.drop(['label'], axis=1)\n",
    "y = transformed_train_data_df['label']\n",
    "\n",
    "train_test_pairs = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_swapped = df_swapped.drop(['label'], axis=1)\n",
    "y_swapped = df_swapped['label']\n",
    "\n",
    "train_test_pairs_swapped = train_test_split(X_swapped, y_swapped, test_size=0.2, random_state=42)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3b. Analyse the performance of the classifiers. Explain which method performs best.\n",
    "From the code below, it is clear that both swapped and unswapped data performs similarly. Classifiers that fail in classification (knn and logistic) perfrom similarly bad on both datasets. However, surprisingly Random Forest works better with unswapped data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import knn classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# KNN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=10, weights='distance', n_jobs=-1)\n",
    "knn_fpr, knn_tpr, knn_roc_auc, _ = train_classifier_and_plot(\n",
    "    knn_classifier, train_test_pairs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic regression\n",
    "logistic_regression_classifier = LogisticRegression(random_state=0)\n",
    "logistic_regression_fpr, logistic_regression_tpr, logistic_regression_roc_auc, _ = train_classifier_and_plot(\n",
    "    logistic_regression_classifier, train_test_pairs\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random forest\n",
    "random_forest_classifier = RandomForestClassifier(random_state=0, class_weight='balanced')\n",
    "random_forest_fpr, random_forest_tpr, random_forest_roc_auc, _ = train_classifier_and_plot(\n",
    "    random_forest_classifier, train_test_pairs\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_roc_curves(\"ROC curves for different blackbox classifiers\",\n",
    "                (knn_fpr, knn_tpr, f\"Gradient Boosted Trees (AUC = {knn_roc_auc:.4f})\"),\n",
    "                (logistic_regression_fpr, logistic_regression_tpr, f\"AdaBoost (AUC = {logistic_regression_roc_auc:.4f})\"),\n",
    "                (random_forest_fpr, random_forest_tpr, f\"Random forest (AUC = {random_forest_roc_auc:.4f})\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyzing the performance of different classifiers on the swapped data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# KNN\u001B[39;00m\n\u001B[0;32m      2\u001B[0m knn_fpr, knn_tpr, knn_roc_auc, _ \u001B[38;5;241m=\u001B[39m train_classifier_and_plot(\n\u001B[1;32m----> 3\u001B[0m     \u001B[43mknn_classifier\u001B[49m, train_test_pairs_swapped\n\u001B[0;32m      4\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'knn_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "knn_fpr, knn_tpr, knn_roc_auc, _ = train_classifier_and_plot(\n",
    "    knn_classifier, train_test_pairs_swapped\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logistic_regression_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Logistic regression\u001B[39;00m\n\u001B[0;32m      2\u001B[0m logistic_regression_fpr, logistic_regression_tpr, logistic_regression_roc_auc, _ \u001B[38;5;241m=\u001B[39m train_classifier_and_plot(\n\u001B[1;32m----> 3\u001B[0m     \u001B[43mlogistic_regression_classifier\u001B[49m, train_test_pairs_swapped\n\u001B[0;32m      4\u001B[0m )\n",
      "\u001B[1;31mNameError\u001B[0m: name 'logistic_regression_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "logistic_regression_fpr, logistic_regression_tpr, logistic_regression_roc_auc, _ = train_classifier_and_plot(\n",
    "    logistic_regression_classifier, train_test_pairs_swapped\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Random forest\n",
    "random_forest_fpr, random_forest_tpr, random_forest_roc_auc, _ = train_classifier_and_plot(\n",
    "random_forest_classifier, train_test_pairs_swapped)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot roc curves\n",
    "plot_roc_curves(\"ROC curves for different blackbox classifiers\",\n",
    "                (knn_fpr, knn_tpr, f\"Gradient Boosted Trees (AUC = {knn_roc_auc:.4f})\"),\n",
    "                (logistic_regression_fpr, logistic_regression_tpr, f\"AdaBoost (AUC = {logistic_regression_roc_auc:.4f})\"),\n",
    "                (random_forest_fpr, random_forest_tpr, f\"Random forest (AUC = {random_forest_roc_auc:.4f})\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Can you explain the performance difference for the different classifiers? Is it advisable to protect people’s privacy using rank-swapping? Why (not)?\n",
    "As you can see, having performed the rank swapping, the differences in performance are only visible for the random forest classifier which surprisingly performs better with the swapped features. This might be due to less overfiting taking place. In the case of other classifiers, the results stay the same. Given these insignificant changes to the performance of the classifier, it is safe to say that rank swapping is a good way to protect people's privacy. However, if the data contains categorical data that cannot be ordered, rank swapping cannot be performed. In such cases, other methods such as adding noise to the data can be used to protect people's privacy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
